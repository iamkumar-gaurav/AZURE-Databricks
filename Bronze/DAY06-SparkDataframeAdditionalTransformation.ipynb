{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0970e4d7-2c7d-485a-8ff7-b5a2b983272e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define source and target file paths for CSV to Parquet conversion\n",
    "# This cell sets the file paths used for reading the source CSV file and writing the output Parquet file.\n",
    "# - sourceFileUrl: Path to the source CSV file in the workspace volume\n",
    "# - targetParquetFilePath: Path to save the converted Parquet file\n",
    "\n",
    "sourceFileUrl = \"/Volumes/workspace/default/lakehouse\"  # Source CSV file path\n",
    "targetParquetFilePath = \"/Volumes/workspace/default/lakehouse/parquet/\"  # Target Parquet file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5e7348-e10f-4a1c-8ee5-6cb8ada49e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file from the specified source path into a Spark DataFrame\n",
    "# The CSV file is loaded without inferring schema and without using the first row as header\n",
    "# sourceFileUrl: Path to the source CSV file in the workspace volume\n",
    "# The resulting DataFrame will have default column names (_c0, _c1, etc.)\n",
    "\n",
    "sourceCSVfileDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .load(sourceFileUrl, format=\"csv\")  # Load CSV file; no header or schema inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd342c6d-5485-4776-ad36-8d4be15f7bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file from the specified source path into a Spark DataFrame\n",
    "# The CSV file is loaded with the first row as header, but without inferring schema\n",
    "\n",
    "sourceCSVfileDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .load(sourceFileUrl, format=\"csv\", header=True)  # Load CSV file with header; no schema inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc2abaeb-4e7c-45ef-aba5-c2433809f0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "sourceCSVFileSchema=StructType([\n",
    "  StructField(\"DATE_OF_PRICING\", StringType(), True),\n",
    "  StructField(\"ROW_ID\", IntegerType(), True),\n",
    "  StructField(\"STATE_NAME\", StringType(), True),\n",
    "  StructField(\"MARKET_NAME\", StringType(), True),\n",
    "  StructField(\"PRODUCTGROUP_NAME\", StringType(), True),\n",
    "  StructField(\"PRODUCT_NAME\", StringType(), True),\n",
    "  StructField(\"VARIETY\", StringType(), True),\n",
    "  StructField(\"ORIGIN\", StringType(), True),\n",
    "  StructField(\"ARRIVAL_IN_TONNES\", DecimalType(10,2), True),\n",
    "  StructField(\"MINIMUM_PRICE\", StringType(), True),\n",
    "  StructField(\"MAXIMUM_PRICE\", StringType(), True),\n",
    "  StructField(\"MODAL_PRICE\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d4e893-d5b8-4035-92e4-c82d856a8016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file from the specified source path into a Spark DataFrame\n",
    "# The CSV file is loaded with the first row as header and schema is inferred automatically\n",
    "\n",
    "sourceCSVfileDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .schema(sourceCSVFileSchema)\n",
    "    .load(\n",
    "        sourceFileUrl,           # Path to the source CSV file\n",
    "        format=\"csv\",            # Specify file format as CSV\n",
    "        header=True,             # Use first row as header\n",
    "        inferSchema=True         # Infer schema automatically from data\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1412db-3012-4bf2-a350-63b5d52112db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col  # Import the 'col' function to reference DataFrame columns\n",
    "\n",
    "# Create a new DataFrame by adding a column 'ARRIVAL_IN_KG' to the source DataFrame\n",
    "# - 'ARRIVAL_IN_KG' is calculated by multiplying 'ARRIVAL_IN_TONNES' by 1000\n",
    "# - This converts the arrival quantity from tonnes to kilograms\n",
    "sourceCSVfileTransDF = (\n",
    "    sourceCSVfileDF.withColumn(\n",
    "        \"ARRIVAL_IN_KG\",              # Name of the new column\n",
    "        col(\"ARRIVAL_IN_TONNES\") * 1000  # Conversion from tonnes to kilograms\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1743aa42-3def-4f4b-aac1-631c1dc36898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the transformed DataFrame with the new 'ARRIVAL_IN_KG' column\n",
    "# - This shows the data after converting 'ARRIVAL_IN_TONNES' to kilograms\n",
    "display(sourceCSVfileTransDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21bc367-cc40-4df5-94ce-26723b43e0a0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767642027361}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *  # Import all functions from pyspark.sql.functions for DataFrame operations\n",
    "\n",
    "# Group the transformed DataFrame by 'PRODUCT_NAME' and 'STATE_NAME'\n",
    "# - This groups the data so that arrival quantities can be aggregated for each product in each state\n",
    "# Aggregate the grouped data by summing the 'ARRIVAL_IN_KG' column for each product and state\n",
    "# - The sum function calculates the total arrival quantity in kilograms for each group\n",
    "# Rename the aggregated column to 'TOTAL_ARRIVAL_IN_KG'\n",
    "# - This provides a clear name for the result of the aggregation\n",
    "# Sort the resulting DataFrame by 'TOTAL_ARRIVAL_IN_KG' in descending order\n",
    "# - This orders the products and states by the highest total arrival quantity first\n",
    "# Display the resulting aggregated and sorted DataFrame\n",
    "display(\n",
    "    sourceCSVfileTransDF\n",
    "        .groupBy(\"PRODUCT_NAME\", \"STATE_NAME\")  # Group by product and state\n",
    "        .agg(sum(\"ARRIVAL_IN_KG\").alias(\"TOTAL_ARRIVAL_IN_KG\"))  # Sum arrival quantity in kilograms for each group\n",
    "        .orderBy(\"TOTAL_ARRIVAL_IN_KG\", ascending=False)  # Sort by total arrival in descending order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386a3914-a579-4337-9155-2a25bdf23398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame 'sourceCSVfileDF' to the specified Parquet file path\n",
    "# - .write: Initiates the write operation for the DataFrame\n",
    "# - .mode(\"overwrite\"): Overwrites any existing data at the target location\n",
    "# - .save(targetParquetFilePath): Saves the DataFrame in Parquet format to the given path\n",
    "sourceCSVfileDF.write.mode(\"overwrite\").save(targetParquetFilePath)  # Save as Parquet file at target path\n",
    "# .saveAsTable(\"sourceCSVfileDF\") can be used to save as a managed table (commented out)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY06-SparkDataframeAdditionalTransformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21816c5e-3667-4453-a628-c37506219aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas  # Import the pandas library for data manipulation and analysis (not used in current Spark workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e772d2d-5643-4da7-beb9-ecf0e09fd0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the source CSV file containing the data to be loaded into a Spark DataFrame\n",
    "sourceFileUrl = \"/Volumes/workspace/default/lakehouse\"\n",
    "\n",
    "# Path to the target directory where the processed data will be saved in JSON format\n",
    "targetFileUrl = \"/Volumes/workspace/default/lakehouse/Json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e7b560-a4d4-4653-bfd2-6abfa854d7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified path into a Spark DataFrame\n",
    "sourceFileDF = (\n",
    "    spark\n",
    "    .read                       # Create a DataFrameReader for loading data\n",
    "    .option(\"header\", \"true\")   # Specify that the CSV file contains a header row with column names\n",
    "    .csv(sourceFileUrl)         # Load the CSV file from the given path into a DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab8056a-2be3-4c3d-ba12-8238c18c8ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`**option(\"header\", \"true\")**` tells PySpark that the CSV file has a header row, so it uses the actual column names from the file instead of assigning default names like `_c0`, `_c1`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a59494c-13db-4051-9d21-fb06a8d488c1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767466365990}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame in a rich tabular format for easy visualization and exploration\n",
    "display(sourceFileDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcf297f-e783-466b-b509-7d1fa410bfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define CSV File Schema\n",
    "This cell defines the schema for the CSV file using `StructType` and `StructField` from `pyspark.sql.types`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64df3350-254a-415f-97c6-af8eccf018bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the CSV file using StructType and StructField from pyspark.sql.types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sourceCSVFileSchema specifies the expected structure and data types for each column in the CSV file\n",
    "sourceCSVFileSchema = sourceFileSchema = StructType([\n",
    "  StructField(\"DATE_OF_PRICING\", StringType(), True),         # Date when pricing was recorded\n",
    "  StructField(\"ROW_ID\", IntegerType(), True),                 # Unique identifier for each row\n",
    "  StructField(\"STATE_NAME\", StringType(), True),              # Name of the state\n",
    "  StructField(\"MARKET_NAME\", StringType(), True),             # Name of the market\n",
    "  StructField(\"PRODUCTGROUP_NAME\", StringType(), True),       # Name of the product group\n",
    "  StructField(\"PRODUCT_NAME\", StringType(), True),            # Name of the product\n",
    "  StructField(\"VARIETY\", StringType(), True),                 # Variety of the product\n",
    "  StructField(\"ORIGIN\", StringType(), True),                  # Origin of the product\n",
    "  StructField(\"ARRIVAL_IN_TONNES\", DecimalType(10,2), True),  # Arrival quantity in tonnes (decimal)\n",
    "  StructField(\"MINIMUM_PRICE\", StringType(), True),           # Minimum price recorded\n",
    "  StructField(\"MAXIMUM_PRICE\", StringType(), True),           # Maximum price recorded\n",
    "  StructField(\"MODAL_PRICE\", StringType(), True)              # Modal price recorded\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328ee450-214f-44d3-9793-eb4f1678ad93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read CSV File into DataFrame\n",
    "This cell reads the CSV file into a Spark DataFrame using the defined schema and displays the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e7aa9b-3aa9-4547-b63d-6c174d2dd919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a Spark DataFrame using the defined schema\n",
    "sourceCSVFileDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .schema(sourceCSVFileSchema)      # Apply the predefined schema to enforce column names and data types\n",
    "    .option(\"header\", \"true\")         # Indicate that the CSV file contains a header row\n",
    "    .csv(sourceFileUrl)               # Specify the path to the CSV file\n",
    ")\n",
    "\n",
    "# Display the DataFrame in a rich tabular format\n",
    "display(sourceCSVFileDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da104628-bbe3-4613-9964-a5b91f30efca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Print DataFrame Schema\n",
    "This cell prints the schema of the DataFrame to verify the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655d83d8-7d28-4cbc-8f1a-cabfcf8be6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the schema of the DataFrame to verify column names and data types\n",
    "sourceCSVFileDF.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY04",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

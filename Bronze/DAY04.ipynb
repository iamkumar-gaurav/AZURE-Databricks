{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21816c5e-3667-4453-a628-c37506219aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e772d2d-5643-4da7-beb9-ecf0e09fd0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sourceFileUrl=\"/Volumes/workspace/default/lakehouse\"\n",
    "targetFileUrl=\"/Volumes/workspace/default/lakehouse/Json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e7b560-a4d4-4653-bfd2-6abfa854d7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sourceFileDF=(spark.\n",
    "              read.\n",
    "              option(\"header\", \"true\").\n",
    "              csv(sourceFileUrl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab8056a-2be3-4c3d-ba12-8238c18c8ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`**option(\"header\", \"true\")**` tells PySpark that the CSV file has a header row, so it uses the actual column names from the file instead of assigning default names like `_c0`, `_c1`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a59494c-13db-4051-9d21-fb06a8d488c1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767466365990}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sourceFileDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcf297f-e783-466b-b509-7d1fa410bfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define CSV File Schema\n",
    "This cell defines the schema for the CSV file using `StructType` and `StructField` from `pyspark.sql.types`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64df3350-254a-415f-97c6-af8eccf018bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "sourceCSVFileSchema=sourceFileSchema=StructType([\n",
    "  StructField(\"DATE_OF_PRICING\", StringType(), True),\n",
    "  StructField(\"ROW_ID\", IntegerType(), True),\n",
    "  StructField(\"STATE_NAME\", StringType(), True),\n",
    "  StructField(\"MARKET_NAME\", StringType(), True),\n",
    "  StructField(\"PRODUCTGROUP_NAME\", StringType(), True),\n",
    "  StructField(\"PRODUCT_NAME\", StringType(), True),\n",
    "  StructField(\"VARIETY\", StringType(), True),\n",
    "  StructField(\"ORIGIN\", StringType(), True),\n",
    "  StructField(\"ARRIVAL_IN_TONNES\", DecimalType(10,2), True),\n",
    "  StructField(\"MINIMUM_PRICE\", StringType(), True),\n",
    "  StructField(\"MAXIMUM_PRICE\", StringType(), True),\n",
    "  StructField(\"MODAL_PRICE\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328ee450-214f-44d3-9793-eb4f1678ad93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read CSV File into DataFrame\n",
    "This cell reads the CSV file into a Spark DataFrame using the defined schema and displays the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e7aa9b-3aa9-4547-b63d-6c174d2dd919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sourceCSVFileDF=(spark.\n",
    "                   read.\n",
    "                   schema(sourceCSVFileSchema).\n",
    "                   option(\"header\", \"true\").\n",
    "                   csv(sourceFileUrl))\n",
    "display(sourceCSVFileDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da104628-bbe3-4613-9964-a5b91f30efca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Print DataFrame Schema\n",
    "This cell prints the schema of the DataFrame to verify the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655d83d8-7d28-4cbc-8f1a-cabfcf8be6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sourceCSVFileDF.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY04",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
